{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fc680470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f926f56",
   "metadata": {},
   "source": [
    "$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(s, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left( \\beta \\left( \\log \\frac{\\pi_\\theta(y_w \\mid s)}{\\pi_{\\text{ref}}(y_w \\mid s)} - \\log \\frac{\\pi_\\theta(y_l \\mid s)}{\\pi_{\\text{ref}}(y_l \\mid s)} \\right) \\right) \\right]\n",
    "$\n",
    "\n",
    "$reward = \\beta \\cdot \\log\\left( \\frac{\\pi_{\\text{ref}}(a \\mid x)}{\\pi_\\theta(a \\mid x)} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2753fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(pi_logps_w, pi_logps_l, ref_logps_w, ref_logps_l, beta):\n",
    "    \"\"\"\n",
    "    pi_logps_w: log πθ(y_w | x), shape [B]\n",
    "    pi_logps_l: log πθ(y_l | x), shape [B]\n",
    "    ref_logps_w: log πref(y_w | x), shape [B]\n",
    "    ref_logps_l: log πref(y_l | x), shape [B]\n",
    "    beta: temperature scaling the KL divergence (float)\n",
    "    \n",
    "    Returns:\n",
    "    - losses: Tensor of shape [B]\n",
    "    - rewards: Diagnostic reward signal, shape [B]\n",
    "    \"\"\"\n",
    "    # KL-adjusted log-ratio difference\n",
    "    pi_logratios = pi_logps_w - pi_logps_l\n",
    "    ref_logratios = ref_logps_w - ref_logps_l\n",
    "\n",
    "    logits = beta * (pi_logratios - ref_logratios)\n",
    "    losses = -F.logsigmoid(logits)\n",
    "\n",
    "    # Diagnostic reward (not used for gradient)\n",
    "    rewards = beta * (pi_logps_w - ref_logps_w).detach()\n",
    "\n",
    "    return losses.mean(), rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f207ef",
   "metadata": {},
   "source": [
    "$\\log \\pi_\\theta(\\tau |s) = \\sum_{t=1}^{T} \\log \\pi_\\theta(a_t \\mid s_t)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#wrapper for policy in DPO \n",
    "#pi: a policy stochastic passed with shape n_states times n_actions\n",
    "class PolicyWrapper_tabular:\n",
    "    def __init__(self, pi):\n",
    "        \"\"\"\n",
    "        pi: Tensor of shape [num_states, num_actions] with action probabilities\n",
    "            Make sure it's a valid probability distribution (rows sum to 1)\n",
    "        \"\"\"\n",
    "        self.pi = pi.clamp(min=1e-8)  # prevent log(0)\n",
    "\n",
    "    def log_prob_trajectory(self, state, trajectories):\n",
    "        \"\"\"\n",
    "        trajectories: List of length B (batch), each a list of (s_t, a_t) pairs\n",
    "        Returns: Tensor of shape [B] with summed log-probabilities per trajectory\n",
    "        \"\"\"\n",
    "        batch_logps = list()\n",
    "        for traj in trajectories:\n",
    "            logps = [torch.log(self.pi[s, a]) for (s, a) in traj]\n",
    "            traj_logp = torch.stack(logps).sum()\n",
    "            batch_logps.append(traj_logp)\n",
    "        return torch.stack(batch_logps)  # shape [B]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9a81285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now suppose policy is NN : \n",
    "\n",
    "class Policy(nn.Module): # definie the policy network\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=32, device='cpu'):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1) # we just consider 1 dimensional probability of action\n",
    "\n",
    "    def predict(self, state, deterministic=True):\n",
    "        \"\"\"\n",
    "        SB3-compatible predict method.\n",
    "        Inputs:\n",
    "            state: np.ndarray or torch.Tensor of shape [state_dim] or [1, state_dim]\n",
    "            deterministic: if True, select action with highest probability\n",
    "        Returns:\n",
    "            action (int), state (None)\n",
    "        \"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        elif isinstance(state, torch.Tensor) and state.dim() == 1:\n",
    "            state = state.unsqueeze(0).float().to(self.device)\n",
    "        else:\n",
    "            state = state.float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = self.forward(state).squeeze(0)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(probs).item()\n",
    "            else:\n",
    "                action = Categorical(probs).sample().item()\n",
    "        return action, None\n",
    "    \n",
    "    def log_prob_trajectory(self, state, trajectories):\n",
    "        logps_batch = list()\n",
    "        for traj in trajectories:\n",
    "            logps = list()\n",
    "            for s, a in traj:\n",
    "                s_tensor = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                probs = self.forward(s_tensor).squeeze(0)\n",
    "                log_prob = torch.log(probs[a])\n",
    "                logps.append(log_prob)\n",
    "            logps_batch.append(torch.stack(logps).sum())\n",
    "        return torch.stack(logps_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pi should be inherit from nn.Module\n",
    "#optimizer should be initialize with pi parameters\n",
    "def update_policy_with_DPO(pi, pref_dataset,nber_epoch, beta, optimizer):\n",
    "    \"\"\"\n",
    "    pi_ref : reference policy\n",
    "    pref_dataset : preference dataset, dataloader that return state , trajectoryw, trajectoryl\n",
    "    nber_epoch : number of epoch\n",
    "    beta the hyperparameter corresponding to the importance of KL div\n",
    "    optimize for the SGD update\n",
    "    return the updated policy with DPO\n",
    "    \"\"\"\n",
    "    pi_ref = copy.deepcopy(pi) # at first pi is initialized to pi_ref\n",
    "    for epoch in range(nber_epoch):\n",
    "        pbar = tqdm(pref_dataset, desc=f\"Epoch {epoch+1}/{nber_epoch}\")\n",
    "        for batch in pbar:\n",
    "            state, t_w, t_l = batch  # Lists of batch_size elements\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # t_w and t_l are List[List[Tuple[state, action]]]\n",
    "            pi_logps_w = pi.log_prob_trajectory(state, t_w)\n",
    "            pi_logps_l = pi.log_prob_trajectory(state, t_l)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pi_ref_logps_w = pi_ref.log_prob_trajectory(state, t_w)\n",
    "                pi_ref_logps_l = pi_ref.log_prob_trajectory(state, t_l)\n",
    "\n",
    "            loss, _ = dpo_loss(pi_logps_w, pi_logps_l, pi_ref_logps_w, pi_ref_logps_l, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Optionally show loss in tqdm\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "683a935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PrefDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Convert initial state to tensor\n",
    "            state = torch.tensor(row['initial_state'], dtype=torch.float32)\n",
    "\n",
    "            # Convert each step in trajectories to (state_tensor, action_tensor)\n",
    "            traj_w = [(torch.tensor(step['state'], dtype=torch.float32),\n",
    "                       torch.tensor(step['action'], dtype=torch.long)) \n",
    "                      for step in row['preferred']]\n",
    "\n",
    "            traj_l = [(torch.tensor(step['state'], dtype=torch.float32),\n",
    "                       torch.tensor(step['action'], dtype=torch.long)) \n",
    "                      for step in row['rejected']]\n",
    "\n",
    "            self.samples.append((state, traj_w, traj_l))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    # batch is a list of tuples: (initial_state_tensor, traj_w_tensor_list, traj_l_tensor_list)\n",
    "    initial_states, traj_ws, traj_ls = zip(*batch)\n",
    "    return list(initial_states), list(traj_ws), list(traj_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "45102f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_6606/3373457478.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  s_tensor = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
      "Epoch 1/20: 100%|██████████| 20/20 [00:03<00:00,  5.38it/s, loss=0.608]\n",
      "Epoch 2/20: 100%|██████████| 20/20 [00:04<00:00,  4.52it/s, loss=0.62] \n",
      "Epoch 3/20: 100%|██████████| 20/20 [00:04<00:00,  4.92it/s, loss=0.568]\n",
      "Epoch 4/20: 100%|██████████| 20/20 [00:03<00:00,  5.23it/s, loss=0.565]\n",
      "Epoch 5/20: 100%|██████████| 20/20 [00:03<00:00,  5.11it/s, loss=0.404]\n",
      "Epoch 6/20: 100%|██████████| 20/20 [00:03<00:00,  5.43it/s, loss=0.533]\n",
      "Epoch 7/20: 100%|██████████| 20/20 [00:03<00:00,  5.42it/s, loss=0.412]\n",
      "Epoch 8/20: 100%|██████████| 20/20 [00:04<00:00,  4.87it/s, loss=0.185]\n",
      "Epoch 9/20: 100%|██████████| 20/20 [00:03<00:00,  5.43it/s, loss=0.259]\n",
      "Epoch 10/20: 100%|██████████| 20/20 [00:03<00:00,  5.07it/s, loss=0.601]\n",
      "Epoch 11/20: 100%|██████████| 20/20 [00:03<00:00,  5.16it/s, loss=0.366]\n",
      "Epoch 12/20: 100%|██████████| 20/20 [00:04<00:00,  4.80it/s, loss=0.589]\n",
      "Epoch 13/20: 100%|██████████| 20/20 [00:03<00:00,  5.14it/s, loss=0.455]\n",
      "Epoch 14/20: 100%|██████████| 20/20 [00:04<00:00,  4.60it/s, loss=0.525]\n",
      "Epoch 15/20: 100%|██████████| 20/20 [00:03<00:00,  5.15it/s, loss=0.229]\n",
      "Epoch 16/20: 100%|██████████| 20/20 [00:03<00:00,  5.60it/s, loss=0.29]  \n",
      "Epoch 17/20: 100%|██████████| 20/20 [00:03<00:00,  5.36it/s, loss=0.166]\n",
      "Epoch 18/20: 100%|██████████| 20/20 [00:03<00:00,  5.19it/s, loss=0.332]\n",
      "Epoch 19/20: 100%|██████████| 20/20 [00:03<00:00,  5.63it/s, loss=0.201]\n",
      "Epoch 20/20: 100%|██████████| 20/20 [00:03<00:00,  5.36it/s, loss=0.163]\n"
     ]
    }
   ],
   "source": [
    "# Load and parse CSV\n",
    "df = pd.read_csv('trajectory_pairs.csv')\n",
    "for col in ['initial_state', 'preferred', 'rejected']:\n",
    "    df[col] = df[col].apply(ast.literal_eval)\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = PrefDataset(df)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "# Initialize policy\n",
    "policy = Policy()\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "# training\n",
    "update_policy_with_DPO(pi=policy, pref_dataset=dataloader,nber_epoch=20, beta=1, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0908280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of evaluation\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset(seed=0)\n",
    "evaluate_policy(policy, env, n_eval_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
