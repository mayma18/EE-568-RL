{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "# User defined imports\n",
    "import tools_RLHF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb85fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"..\\\\PPO_Original\\\\Training\\\\2025-05-15_22-16-35\\\\RLHF_trajectory_pairs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fac82",
   "metadata": {},
   "source": [
    "**check the total reward (in cartpole environment, total reward is the same as episode length) of preferred and rejected trajectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "0 500 44\n",
      "1 500 175\n",
      "2 500 78\n",
      "3 500 41\n",
      "4 500 116\n",
      "5 500 103\n",
      "6 500 60\n",
      "7 500 60\n",
      "8 500 91\n",
      "9 500 13\n"
     ]
    }
   ],
   "source": [
    "# path = \"trajectory_pairs.csv\"\n",
    "Data = tools_RLHF.Data_Class(path)\n",
    "for i in range(10):\n",
    "    print(i, len(Data.trajs_prefer_list.get_single_traj_json(i)), len(Data.trajs_reject_list.get_single_traj_json(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Reward Model Training**\n",
    "use `trajectory_pairs.csv` to train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/100 ‚Äî Avg Loss: 0.7466\n",
      "Epoch 2/100 ‚Äî Avg Loss: 0.6167\n",
      "Epoch 3/100 ‚Äî Avg Loss: 0.5279\n",
      "Epoch 4/100 ‚Äî Avg Loss: 0.4635\n",
      "Epoch 5/100 ‚Äî Avg Loss: 0.4184\n",
      "Epoch 6/100 ‚Äî Avg Loss: 0.3800\n",
      "Epoch 7/100 ‚Äî Avg Loss: 0.3549\n",
      "Epoch 8/100 ‚Äî Avg Loss: 0.3379\n",
      "Epoch 9/100 ‚Äî Avg Loss: 0.3243\n",
      "Epoch 10/100 ‚Äî Avg Loss: 0.3150\n",
      "Epoch 11/100 ‚Äî Avg Loss: 0.3068\n",
      "Epoch 12/100 ‚Äî Avg Loss: 0.3004\n",
      "Epoch 13/100 ‚Äî Avg Loss: 0.2949\n",
      "Epoch 14/100 ‚Äî Avg Loss: 0.2888\n",
      "Epoch 15/100 ‚Äî Avg Loss: 0.2868\n",
      "Epoch 16/100 ‚Äî Avg Loss: 0.2829\n",
      "Epoch 17/100 ‚Äî Avg Loss: 0.2776\n",
      "Epoch 18/100 ‚Äî Avg Loss: 0.2744\n",
      "Epoch 19/100 ‚Äî Avg Loss: 0.2735\n",
      "Epoch 20/100 ‚Äî Avg Loss: 0.2711\n",
      "Epoch 21/100 ‚Äî Avg Loss: 0.2682\n",
      "Epoch 22/100 ‚Äî Avg Loss: 0.2658\n",
      "Epoch 23/100 ‚Äî Avg Loss: 0.2644\n",
      "Epoch 24/100 ‚Äî Avg Loss: 0.2608\n",
      "Epoch 25/100 ‚Äî Avg Loss: 0.2589\n",
      "Epoch 26/100 ‚Äî Avg Loss: 0.2576\n",
      "Epoch 27/100 ‚Äî Avg Loss: 0.2553\n",
      "Epoch 28/100 ‚Äî Avg Loss: 0.2530\n",
      "Epoch 29/100 ‚Äî Avg Loss: 0.2526\n",
      "Epoch 30/100 ‚Äî Avg Loss: 0.2510\n",
      "Epoch 31/100 ‚Äî Avg Loss: 0.2504\n",
      "Epoch 32/100 ‚Äî Avg Loss: 0.2484\n",
      "Epoch 33/100 ‚Äî Avg Loss: 0.2463\n",
      "Epoch 34/100 ‚Äî Avg Loss: 0.2450\n",
      "Epoch 35/100 ‚Äî Avg Loss: 0.2453\n",
      "Epoch 36/100 ‚Äî Avg Loss: 0.2426\n",
      "Epoch 37/100 ‚Äî Avg Loss: 0.2409\n",
      "Epoch 38/100 ‚Äî Avg Loss: 0.2400\n",
      "Epoch 39/100 ‚Äî Avg Loss: 0.2382\n",
      "Epoch 40/100 ‚Äî Avg Loss: 0.2392\n",
      "Epoch 41/100 ‚Äî Avg Loss: 0.2365\n",
      "Epoch 42/100 ‚Äî Avg Loss: 0.2345\n",
      "Epoch 43/100 ‚Äî Avg Loss: 0.2338\n",
      "Epoch 44/100 ‚Äî Avg Loss: 0.2330\n",
      "Epoch 45/100 ‚Äî Avg Loss: 0.2318\n",
      "Epoch 46/100 ‚Äî Avg Loss: 0.2301\n",
      "Epoch 47/100 ‚Äî Avg Loss: 0.2301\n",
      "Epoch 48/100 ‚Äî Avg Loss: 0.2286\n",
      "Epoch 49/100 ‚Äî Avg Loss: 0.2275\n",
      "Epoch 50/100 ‚Äî Avg Loss: 0.2270\n",
      "Epoch 51/100 ‚Äî Avg Loss: 0.2253\n",
      "Epoch 52/100 ‚Äî Avg Loss: 0.2248\n",
      "Epoch 53/100 ‚Äî Avg Loss: 0.2236\n",
      "Epoch 54/100 ‚Äî Avg Loss: 0.2244\n",
      "Epoch 55/100 ‚Äî Avg Loss: 0.2245\n",
      "Epoch 56/100 ‚Äî Avg Loss: 0.2239\n",
      "Epoch 57/100 ‚Äî Avg Loss: 0.2221\n",
      "Epoch 58/100 ‚Äî Avg Loss: 0.2205\n",
      "Epoch 59/100 ‚Äî Avg Loss: 0.2184\n",
      "Epoch 60/100 ‚Äî Avg Loss: 0.2186\n",
      "Epoch 61/100 ‚Äî Avg Loss: 0.2169\n",
      "Epoch 62/100 ‚Äî Avg Loss: 0.2148\n",
      "Epoch 63/100 ‚Äî Avg Loss: 0.2168\n",
      "Epoch 64/100 ‚Äî Avg Loss: 0.2156\n",
      "Epoch 65/100 ‚Äî Avg Loss: 0.2138\n",
      "Epoch 66/100 ‚Äî Avg Loss: 0.2132\n",
      "Epoch 67/100 ‚Äî Avg Loss: 0.2134\n",
      "Epoch 68/100 ‚Äî Avg Loss: 0.2116\n",
      "Epoch 69/100 ‚Äî Avg Loss: 0.2106\n",
      "Epoch 70/100 ‚Äî Avg Loss: 0.2105\n",
      "Epoch 71/100 ‚Äî Avg Loss: 0.2099\n",
      "Epoch 72/100 ‚Äî Avg Loss: 0.2090\n",
      "Epoch 73/100 ‚Äî Avg Loss: 0.2086\n",
      "Epoch 74/100 ‚Äî Avg Loss: 0.2077\n",
      "Epoch 75/100 ‚Äî Avg Loss: 0.2069\n",
      "Epoch 76/100 ‚Äî Avg Loss: 0.2069\n",
      "Epoch 77/100 ‚Äî Avg Loss: 0.2058\n",
      "Epoch 78/100 ‚Äî Avg Loss: 0.2057\n",
      "Epoch 79/100 ‚Äî Avg Loss: 0.2048\n",
      "Epoch 80/100 ‚Äî Avg Loss: 0.2055\n",
      "Epoch 81/100 ‚Äî Avg Loss: 0.2041\n",
      "Epoch 82/100 ‚Äî Avg Loss: 0.2040\n",
      "Epoch 83/100 ‚Äî Avg Loss: 0.2020\n",
      "Epoch 84/100 ‚Äî Avg Loss: 0.2021\n",
      "Epoch 85/100 ‚Äî Avg Loss: 0.2035\n",
      "Epoch 86/100 ‚Äî Avg Loss: 0.2039\n",
      "Epoch 87/100 ‚Äî Avg Loss: 0.2024\n",
      "Epoch 88/100 ‚Äî Avg Loss: 0.2001\n",
      "Epoch 89/100 ‚Äî Avg Loss: 0.2002\n",
      "Epoch 90/100 ‚Äî Avg Loss: 0.1989\n",
      "Epoch 91/100 ‚Äî Avg Loss: 0.1986\n",
      "Epoch 92/100 ‚Äî Avg Loss: 0.1985\n",
      "Epoch 93/100 ‚Äî Avg Loss: 0.1977\n",
      "Epoch 94/100 ‚Äî Avg Loss: 0.1974\n",
      "Epoch 95/100 ‚Äî Avg Loss: 0.1974\n",
      "Epoch 96/100 ‚Äî Avg Loss: 0.1968\n",
      "Epoch 97/100 ‚Äî Avg Loss: 0.1963\n",
      "Epoch 98/100 ‚Äî Avg Loss: 0.1960\n",
      "Epoch 99/100 ‚Äî Avg Loss: 0.1952\n",
      "Epoch 100/100 ‚Äî Avg Loss: 0.1955\n",
      "üéâ Model has been saved to: reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî Hyperparameters ‚Äî‚Äî‚Äî‚Äî\n",
    "gamma = 0.99      \n",
    "lr = 1e-4          \n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî Load Data ‚Äî‚Äî‚Äî‚Äî\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = tools_RLHF.Data_Class(path)\n",
    "\n",
    "# Ëá™ÂÆö‰πâ collate_fnÔºå‰øùÁïôÂèòÈïøÂ∫èÂàó\n",
    "# Custom collate_fn to keep variable-length sequences\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Training Preperation ‚Äî‚Äî‚Äî\n",
    "dataset = tools_RLHF.PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = tools_RLHF.RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64) # ÂÆû‰æãÂåñ Á•ûÁªèÁΩëÁªú MLP\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Training Loop ‚Äî‚Äî‚Äî\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # ËÆ°ÁÆó prefer ËΩ®ËøπÁöÑÂõûÊä•\n",
    "        # Calculate the return for preferred trajectories\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # ËÆ°ÁÆó reject ËΩ®ËøπÁöÑÂõûÊä•\n",
    "        # Calculate the return for rejected trajectories\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        \n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} ‚Äî Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî Save Model ‚Äî‚Äî‚Äî\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"üéâ Model has been saved to: reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Reward Model Testing**\n",
    "Load `trajectory_pairs.csv` to see if the total reward matches what the `MLP reward model` predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(20.7971) tensor(19.7983)\n",
      "1 tensor(19.7432) tensor(16.6396)\n",
      "2 tensor(21.2318) tensor(19.4247)\n",
      "3 tensor(20.6215) tensor(18.8928)\n",
      "4 tensor(21.0973) tensor(12.7524)\n",
      "5 tensor(21.1557) tensor(13.4750)\n",
      "6 tensor(19.6617) tensor(11.9227)\n",
      "7 tensor(20.5343) tensor(11.9880)\n",
      "8 tensor(19.8585) tensor(18.6292)\n",
      "9 tensor(19.8084) tensor(13.6240)\n"
     ]
    }
   ],
   "source": [
    "# ‚Äî‚Äî‚Äî Load MLP Reward Model ‚Äî‚Äî‚Äî\n",
    "reward_net_loaded = tools_RLHF.RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "reward_net_loaded.eval()\n",
    "\n",
    "# Load .csv Data\n",
    "for i in range(10):\n",
    "    traj_prefer_json = Data.trajs_prefer_list.get_single_traj_json(i)\n",
    "    traj_reject_json = Data.trajs_reject_list.get_single_traj_json(i)\n",
    "\n",
    "    # convert to tensor\n",
    "    states_prefer  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                  for step in traj_prefer_json], dim=0)    # [L, s_dim]\n",
    "    actions_prefer = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_prefer_json], dim=0)  # [L, a_dim]\n",
    "    states_reject  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, s_dim]\n",
    "    actions_reject = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "    # ËÆ°ÁÆó prefer ËΩ®ËøπÁöÑÂõûÊä•\n",
    "    # Calculate the return for preferred trajectories\n",
    "    with torch.no_grad():\n",
    "        r_pref = reward_net_loaded(states_prefer, actions_prefer)          \n",
    "\n",
    "    # ËÆ°ÁÆóÊÄªÂõûÊä•\n",
    "    # Calculate total return\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_pref.size(0))])\n",
    "    total_return_prefer = (r_pref * discounts).sum()\n",
    "\n",
    "    # ËÆ°ÁÆó reject ËΩ®ËøπÁöÑÂõûÊä•\n",
    "    # Calculate the return for rejected trajectories\n",
    "    with torch.no_grad():\n",
    "        r_rj = reward_net_loaded(states_reject, actions_reject)          \n",
    "\n",
    "    # ËÆ°ÁÆóÊÄªÂõûÊä•\n",
    "    # Calculate total return\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))])\n",
    "    total_return_reject = (r_rj * discounts).sum()\n",
    "\n",
    "    print(i, total_return_prefer, total_return_reject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951354",
   "metadata": {},
   "source": [
    "## **PPO-RLHF Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\2025-05-15_22-30-31\\PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.2     |\n",
      "|    ep_rew_mean     | 5.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 1804     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 28.5         |\n",
      "|    ep_rew_mean          | 6.54         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 989          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116936825 |\n",
      "|    clip_fraction        | 0.142        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.0034       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.294        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0191      |\n",
      "|    value_loss           | 1.76         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.7        |\n",
      "|    ep_rew_mean          | 8.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 831         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014814891 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.788       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 48.8        |\n",
      "|    ep_rew_mean          | 10.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008453388 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.5        |\n",
      "|    ep_rew_mean          | 13.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006942897 |\n",
      "|    clip_fraction        | 0.0836      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.702       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 70.5        |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 782         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009798845 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.438       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.2        |\n",
      "|    ep_rew_mean          | 19.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012045563 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.378       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | 23.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 754          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066338964 |\n",
      "|    clip_fraction        | 0.0744       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.117        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 1.22         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 116         |\n",
      "|    ep_rew_mean          | 24.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010805077 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.297       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | 29.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 742          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077515505 |\n",
      "|    clip_fraction        | 0.099        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.271        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00773     |\n",
      "|    value_loss           | 0.979        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 155          |\n",
      "|    ep_rew_mean          | 33.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 739          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051640766 |\n",
      "|    clip_fraction        | 0.0574       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.533       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.473        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00721     |\n",
      "|    value_loss           | 0.589        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 172          |\n",
      "|    ep_rew_mean          | 38.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 736          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045594545 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.368        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.875        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00351     |\n",
      "|    value_loss           | 1.34         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 190       |\n",
      "|    ep_rew_mean          | 43.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 737       |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 36        |\n",
      "|    total_timesteps      | 26624     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0078364 |\n",
      "|    clip_fraction        | 0.069     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.498    |\n",
      "|    explained_variance   | 0.836     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00817  |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.00378  |\n",
      "|    value_loss           | 0.148     |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 213          |\n",
      "|    ep_rew_mean          | 48.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 734          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045226584 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0098       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    value_loss           | 0.0573       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 227          |\n",
      "|    ep_rew_mean          | 51.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 732          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059407777 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00656     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    value_loss           | 0.0382       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 253          |\n",
      "|    ep_rew_mean          | 58.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 731          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049920985 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.486       |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00205     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00545     |\n",
      "|    value_loss           | 0.0331       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 262          |\n",
      "|    ep_rew_mean          | 60.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 727          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028570532 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0155       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 0.0327       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 288          |\n",
      "|    ep_rew_mean          | 67.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033700461 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00547      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 0.0274       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 301         |\n",
      "|    ep_rew_mean          | 70.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 725         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005200169 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    value_loss           | 0.025       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 320          |\n",
      "|    ep_rew_mean          | 76.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 725          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041772407 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0219      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00394     |\n",
      "|    value_loss           | 0.0158       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 333          |\n",
      "|    ep_rew_mean          | 80           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 722          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015022854 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.465       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00656      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000337    |\n",
      "|    value_loss           | 0.0151       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 356          |\n",
      "|    ep_rew_mean          | 86.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 719          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037581844 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00132      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | 6.27e-06     |\n",
      "|    value_loss           | 0.0148       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 369          |\n",
      "|    ep_rew_mean          | 90.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 718          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022574188 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00412     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 0.015        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 390         |\n",
      "|    ep_rew_mean          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002496263 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0011     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    value_loss           | 0.011       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 407          |\n",
      "|    ep_rew_mean          | 101          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 713          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036688591 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00299      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 0.00761      |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# 2. ÂÆö‰πâ‰∏Ä‰∏™ WrapperÔºåÂú® step ÈáåÁî®‰Ω†ÁöÑ MLP ËÆ°ÁÆó reward\n",
    "# 2. Define a Wrapper that uses your MLP to calculate the reward in step\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_model_path, device=\"cpu\"):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # state dimension\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "\n",
    "        # action dimension\n",
    "        try:                  self.dim_action = env.action_space.shape[0]\n",
    "        except IndexError:    self.dim_action = 1\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Ensure the input dimensions match the checkpoint\n",
    "        checkpoint = torch.load(reward_model_path, map_location=device, weights_only=False)\n",
    "        input_dim = checkpoint['net.0.weight'].size(1)  # Extract input size from checkpoint\n",
    "        self.reward_model = tools_RLHF.RewardMLP(input_dim - self.dim_action, self.dim_action).to(device)\n",
    "        self.reward_model.load_state_dict(checkpoint)\n",
    "        self.reward_model.load_state_dict(torch.load(reward_model_path, map_location=device, weights_only=False))\n",
    "        self.reward_model.eval()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # ÊâßË°åÂéü envÔºå‰∏çÁî®Âéü reward\n",
    "        # Execute the original env, without the original reward\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)   \n",
    "        \n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # ‰øÆÊîπ action_tensor ÂΩ¢Áä∂\n",
    "        # Modify action_tensor shape\n",
    "        if action_tensor.ndim == 1:\n",
    "            action_tensor = action_tensor.view(1, -1)\n",
    "\n",
    "        # ËÆ°ÁÆóÂ•ñÂä±  \n",
    "        # Calculate reward\n",
    "        with torch.no_grad():\n",
    "            reward_tensor = self.reward_model(state_tensor, \n",
    "                                              action_tensor)\n",
    "        reward = reward_tensor.item()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "def reset(self, **kwargs):\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# 3. ÊûÑÈÄ† vectorized ÁéØÂ¢ÉÔºåÂπ∂Â∫îÁî®Ëá™ÂÆö‰πâ Wrapper\n",
    "# 3. Construct vectorized environment and apply custom Wrapper\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Log path\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = os.path.join(\"Training\", current_time)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Reward Model path\n",
    "MODEL_PATH = \"reward_net.pth\"\n",
    "\n",
    "\n",
    "#### Build env\n",
    "vec_env = make_vec_env(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_envs=8,\n",
    "    wrapper_class=lambda env: CustomRewardWrapper(env, MODEL_PATH, device=\"cpu\"),\n",
    "    monitor_dir=log_path\n",
    ")\n",
    "\n",
    "# vec_env = VecMonitor(vec_env, log_path)\n",
    "\n",
    "\n",
    "#### Build PPO Model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=256,\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path\n",
    ")\n",
    "\n",
    "#### Training\n",
    "model.learn( \n",
    "    total_timesteps=50000,\n",
    "    # callback=[eval_callback, save_callback]\n",
    ")\n",
    "\n",
    "#### Save Model\n",
    "model.save(os.path.join(log_path, \"model_full_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01aac6",
   "metadata": {},
   "source": [
    "## **PPO-RLHF Testing**\n",
    "You can find record video in `Training\\2025-xx-xx\\video`  \n",
    "‰Ω†ÂèØ‰ª•Âú®Ëøô‰∏™‰ΩçÁΩÆÊâæÂà∞ÂΩïÂÉè `Training\\2025-xx-xx\\video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01c1061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_path: Training\\2025-05-15_22-30-31\n",
      "Training\\2025-05-15_22-30-31\\model_full_training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\EPFL\\Course\\MA2\\[EE-568] Reinforcement Learning\\EE-568-RL\\PPO_RLHF\\Training\\2025-05-15_22-30-31\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 500.0\n",
      "Episode: 2 Score: 500.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "cur_dir = os.getcwd()\n",
    "pkg_dir = os.path.dirname(cur_dir)\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.append(pkg_dir)\n",
    "from PPO_Original import tools\n",
    "\n",
    "\n",
    "# PPO-RLHF model testing\n",
    "# log_path = \"Training\\\\2025-05-08_19-59-36\" #0000FF Change this to your log path\n",
    "print(\"log_path:\", log_path)\n",
    "PPO_Model_Path = os.path.join(log_path, \"model_full_training\")\n",
    "tools.test_model(\"PPO\", PPO_Model_Path, n_episodes=2, render = True, record=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
