{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514fce1",
   "metadata": {},
   "source": [
    "## **User Define Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5849e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Class:\n",
    "\n",
    "    # Internal Class\n",
    "    class Trajectory_Class:\n",
    "        def __init__(self, traj_series):\n",
    "            self.traj_list = traj_series\n",
    "            self.length = len(traj_series)\n",
    "\n",
    "        def get_single_traj(self, index):\n",
    "            return json.loads(self.traj_list[index])\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        # åŸå§‹æ•°æ® Original Data\n",
    "        self.trajs_prefer_list = []\n",
    "        self.trajs_reject_list = []\n",
    "\n",
    "        # å¤„ç†æ•°æ® Processed Data\n",
    "        self.traj_prefer_list_list_tensor = []\n",
    "        self.traj_reject_list_list_tensor = []\n",
    "\n",
    "        # å¯åŠ¨å‡½æ•° Start Function\n",
    "        self.load_data(path)\n",
    "        self.convert(self.trajs_prefer_list, self.traj_prefer_list_list_tensor)   # data convert æ•°æ®è½¬æ¢\n",
    "        self.convert(self.trajs_reject_list, self.traj_reject_list_list_tensor)\n",
    "        print(\"Data loaded successfully\")\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        self.trajs_prefer_list = Data_Class.Trajectory_Class(data['preferred'])   # list data æ•°æ®\n",
    "        self.trajs_reject_list = Data_Class.Trajectory_Class(data['rejected'])    # list data æ•°æ®\n",
    "\n",
    "    def convert(self,\n",
    "                list_json: Trajectory_Class,\n",
    "                traj_list_list_tensor):\n",
    "\n",
    "        # è·å–ç¬¬0æ¡è½¨è¿¹çš„ç¬¬0æ—¶åˆ»æ ·æœ¬æ¥ç¡®å®šç»´åº¦\n",
    "        # Get the first sample of the first trajectory to determine dimensions\n",
    "        sample = list_json.get_single_traj(0)[0]\n",
    "        state0 = np.array(sample['state'])\n",
    "        action0 = np.array(sample['action'])\n",
    "\n",
    "        # è·å– state action ç»´åº¦\n",
    "        # Get the dimensions of state and action\n",
    "        self.dim_state = state0.size if state0.ndim == 0 else state0.shape[0]\n",
    "        self.dim_action = action0.size if action0.ndim == 0 else action0.shape[0]\n",
    "\n",
    "        # æ•°æ®æ‰¹é‡è½¬æ¢ tensor\n",
    "        # Convert data to tensor in batches\n",
    "        for idx in range(list_json.length):\n",
    "            traj = list_json.get_single_traj(idx)\n",
    "            states, actions = [], []\n",
    "\n",
    "            for time_i in traj:\n",
    "                # è½¬æ¢ä¸º numpyï¼Œç„¶å torch tensor\n",
    "                # Convert to numpy, then torch tensor\n",
    "                state_np = np.array(time_i['state'])\n",
    "                action_np = np.array(time_i['action'])\n",
    "\n",
    "                state_t = torch.from_numpy(state_np).float()\n",
    "                action_t = torch.from_numpy(action_np).float()\n",
    "\n",
    "                # å¦‚æœæ˜¯ä¸€ç»´æ ‡é‡ï¼Œè¦å±•å¼€æˆé•¿åº¦1å‘é‡\n",
    "                # If it's a one-dimensional scalar, expand it into a length 1 vector\n",
    "                state_t = state_t.view(-1)\n",
    "                action_t = action_t.view(-1)\n",
    "\n",
    "                states.append(state_t)\n",
    "                actions.append(action_t)\n",
    "\n",
    "            # å°†åˆ—è¡¨å †æˆå¼ é‡ [L_i, dim]\n",
    "            # Stack the list into a tensor [L_i, dim]\n",
    "            states_tensor = torch.stack(states, dim=0)\n",
    "            actions_tensor = torch.stack(actions, dim=0)\n",
    "\n",
    "            # å°†æ¯æ¡è½¨è¿¹ä½œä¸ºä¸€ä¸ªå…ƒç»„ (states, actions) æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "            # Add each trajectory as a tuple (states, actions) to the list\n",
    "            traj_list_list_tensor.append((states_tensor, actions_tensor))\n",
    "\n",
    "# â€”â€”â€” æ•°æ®é›†ä¸åŠ è½½å™¨ â€”â€”â€”\n",
    "# Dataset and DataLoader\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, pref, rej, gamma):\n",
    "        assert len(pref) == len(rej)\n",
    "        self.pref = pref\n",
    "        self.rej = rej\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.pref[idx], *self.rej[idx])\n",
    "\n",
    "# MLP æ‰“åˆ†ç±» \n",
    "# MLP scoring model class\n",
    "class RewardMLP(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #0000FF # è¿™é‡Œåœ¨æ„é€ ç¥ç»ç½‘ç»œï¼Œåå»å¯èƒ½éœ€è¦è°ƒæ•´ç¥ç»ç½‘ç»œç»“æ„ \n",
    "            #0000FF # Here we construct the neural network, which may need to be adjusted later \n",
    "            nn.Linear(s_dim + a_dim, hidden_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [L_i, s_dim], a: [L_i, a_dim]\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fac82",
   "metadata": {},
   "source": [
    "**check the total reward (in cartpole environment, total reward is the same as episode length) of preferred and rejected trajectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "0 200 200\n",
      "1 200 144\n",
      "2 200 180\n",
      "3 200 194\n",
      "4 200 96\n",
      "5 200 125\n",
      "6 200 108\n",
      "7 200 106\n",
      "8 200 166\n",
      "9 173 97\n"
     ]
    }
   ],
   "source": [
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, len(Data.trajs_prefer_list.get_single_traj(i)), len(Data.trajs_reject_list.get_single_traj(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Reward Model Training**\n",
    "use `trajectory_pairs.csv` to train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/50 â€” Avg Loss: 1.0871\n",
      "Epoch 2/50 â€” Avg Loss: 0.9674\n",
      "Epoch 3/50 â€” Avg Loss: 0.8635\n",
      "Epoch 4/50 â€” Avg Loss: 0.7858\n",
      "Epoch 5/50 â€” Avg Loss: 0.7040\n",
      "Epoch 6/50 â€” Avg Loss: 0.6431\n",
      "Epoch 7/50 â€” Avg Loss: 0.5810\n",
      "Epoch 8/50 â€” Avg Loss: 0.5335\n",
      "Epoch 9/50 â€” Avg Loss: 0.4875\n",
      "Epoch 10/50 â€” Avg Loss: 0.4552\n",
      "Epoch 11/50 â€” Avg Loss: 0.4271\n",
      "Epoch 12/50 â€” Avg Loss: 0.4042\n",
      "Epoch 13/50 â€” Avg Loss: 0.3839\n",
      "Epoch 14/50 â€” Avg Loss: 0.3686\n",
      "Epoch 15/50 â€” Avg Loss: 0.3566\n",
      "Epoch 16/50 â€” Avg Loss: 0.3452\n",
      "Epoch 17/50 â€” Avg Loss: 0.3359\n",
      "Epoch 18/50 â€” Avg Loss: 0.3289\n",
      "Epoch 19/50 â€” Avg Loss: 0.3226\n",
      "Epoch 20/50 â€” Avg Loss: 0.3161\n",
      "Epoch 21/50 â€” Avg Loss: 0.3112\n",
      "Epoch 22/50 â€” Avg Loss: 0.3068\n",
      "Epoch 23/50 â€” Avg Loss: 0.3024\n",
      "Epoch 24/50 â€” Avg Loss: 0.2987\n",
      "Epoch 25/50 â€” Avg Loss: 0.2969\n",
      "Epoch 26/50 â€” Avg Loss: 0.2935\n",
      "Epoch 27/50 â€” Avg Loss: 0.2911\n",
      "Epoch 28/50 â€” Avg Loss: 0.2891\n",
      "Epoch 29/50 â€” Avg Loss: 0.2867\n",
      "Epoch 30/50 â€” Avg Loss: 0.2839\n",
      "Epoch 31/50 â€” Avg Loss: 0.2812\n",
      "Epoch 32/50 â€” Avg Loss: 0.2781\n",
      "Epoch 33/50 â€” Avg Loss: 0.2779\n",
      "Epoch 34/50 â€” Avg Loss: 0.2750\n",
      "Epoch 35/50 â€” Avg Loss: 0.2729\n",
      "Epoch 36/50 â€” Avg Loss: 0.2733\n",
      "Epoch 37/50 â€” Avg Loss: 0.2694\n",
      "Epoch 38/50 â€” Avg Loss: 0.2674\n",
      "Epoch 39/50 â€” Avg Loss: 0.2663\n",
      "Epoch 40/50 â€” Avg Loss: 0.2655\n",
      "Epoch 41/50 â€” Avg Loss: 0.2637\n",
      "Epoch 42/50 â€” Avg Loss: 0.2625\n",
      "Epoch 43/50 â€” Avg Loss: 0.2617\n",
      "Epoch 44/50 â€” Avg Loss: 0.2593\n",
      "Epoch 45/50 â€” Avg Loss: 0.2576\n",
      "Epoch 46/50 â€” Avg Loss: 0.2559\n",
      "Epoch 47/50 â€” Avg Loss: 0.2553\n",
      "Epoch 48/50 â€” Avg Loss: 0.2537\n",
      "Epoch 49/50 â€” Avg Loss: 0.2522\n",
      "Epoch 50/50 â€” Avg Loss: 0.2512\n",
      "ğŸ‰ Model has been saved to: reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â€”â€”â€”â€” Hyperparameters â€”â€”â€”â€”\n",
    "gamma = 0.99      \n",
    "lr = 1e-4          \n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "# â€”â€”â€”â€” Load Data â€”â€”â€”â€”\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "# è‡ªå®šä¹‰ collate_fnï¼Œä¿ç•™å˜é•¿åºåˆ—\n",
    "# Custom collate_fn to keep variable-length sequences\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# â€”â€”â€” Training Preperation â€”â€”â€”\n",
    "dataset = PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64) # å®ä¾‹åŒ– ç¥ç»ç½‘ç»œ MLP\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# â€”â€”â€” Training Loop â€”â€”â€”\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # è®¡ç®— prefer è½¨è¿¹çš„å›æŠ¥\n",
    "        # Calculate the return for preferred trajectories\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # è®¡ç®— reject è½¨è¿¹çš„å›æŠ¥\n",
    "        # Calculate the return for rejected trajectories\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        \n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} â€” Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# â€”â€”â€” Save Model â€”â€”â€”\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"ğŸ‰ Model has been saved to: reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Reward Model Testing**\n",
    "Load `trajectory_pairs.csv` to see if the total reward matches what the `MLP reward model` predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(12.2832) tensor(11.5129)\n",
      "1 tensor(11.5982) tensor(8.9034)\n",
      "2 tensor(12.5000) tensor(10.9671)\n",
      "3 tensor(12.3189) tensor(10.8672)\n",
      "4 tensor(12.3967) tensor(6.5947)\n",
      "5 tensor(12.2223) tensor(8.9502)\n",
      "6 tensor(12.2360) tensor(6.0135)\n",
      "7 tensor(12.1494) tensor(5.9533)\n",
      "8 tensor(11.9423) tensor(11.1330)\n",
      "9 tensor(11.5264) tensor(7.4788)\n"
     ]
    }
   ],
   "source": [
    "# â€”â€”â€” Load MLP Reward Model â€”â€”â€”\n",
    "reward_net_loaded = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "reward_net_loaded.eval()\n",
    "\n",
    "# Load .csv Data\n",
    "for i in range(10):\n",
    "    traj_prefer_json = Data.trajs_prefer_list.get_single_traj(i)\n",
    "    traj_reject_json = Data.trajs_reject_list.get_single_traj(i)\n",
    "\n",
    "    # convert to tensor\n",
    "    states_prefer  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                  for step in traj_prefer_json], dim=0)    # [L, s_dim]\n",
    "    actions_prefer = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_prefer_json], dim=0)  # [L, a_dim]\n",
    "    states_reject  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, s_dim]\n",
    "    actions_reject = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "    # è®¡ç®— prefer è½¨è¿¹çš„å›æŠ¥\n",
    "    # Calculate the return for preferred trajectories\n",
    "    with torch.no_grad():\n",
    "        r_pref = reward_net_loaded(states_prefer, actions_prefer)          \n",
    "\n",
    "    # è®¡ç®—æ€»å›æŠ¥\n",
    "    # Calculate total return\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_pref.size(0))])\n",
    "    total_return_prefer = (r_pref * discounts).sum()\n",
    "\n",
    "    # è®¡ç®— reject è½¨è¿¹çš„å›æŠ¥\n",
    "    # Calculate the return for rejected trajectories\n",
    "    with torch.no_grad():\n",
    "        r_rj = reward_net_loaded(states_reject, actions_reject)          \n",
    "\n",
    "    # è®¡ç®—æ€»å›æŠ¥\n",
    "    # Calculate total return\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))])\n",
    "    total_return_reject = (r_rj * discounts).sum()\n",
    "\n",
    "    print(i, total_return_prefer, total_return_reject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951354",
   "metadata": {},
   "source": [
    "## **PPO-RLHF Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\2025-05-08_21-21-23\\PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | 3.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 3437     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.6        |\n",
      "|    ep_rew_mean          | 3.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1770        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010768814 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.0244      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.522       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.4        |\n",
      "|    ep_rew_mean          | 5.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1460        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014078374 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.181       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.535       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 45.6        |\n",
      "|    ep_rew_mean          | 6.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1372        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011568268 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.814       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 59.8         |\n",
      "|    ep_rew_mean          | 8.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1313         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075433906 |\n",
      "|    clip_fraction        | 0.0745       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.294        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.758        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73.9        |\n",
      "|    ep_rew_mean          | 10.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1286        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011101447 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.471       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 82.6        |\n",
      "|    ep_rew_mean          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1262        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015353655 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0939      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# 2. å®šä¹‰ä¸€ä¸ª Wrapperï¼Œåœ¨ step é‡Œç”¨ä½ çš„ MLP è®¡ç®— reward\n",
    "# 2. Define a Wrapper that uses your MLP to calculate the reward in step\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_model_path, device=\"cpu\"):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # state dimension\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "\n",
    "        # action dimension\n",
    "        try:                  self.dim_action = env.action_space.shape[0]\n",
    "        except IndexError:    self.dim_action = 1\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Ensure the input dimensions match the checkpoint\n",
    "        checkpoint = torch.load(reward_model_path, map_location=device, weights_only=False)\n",
    "        input_dim = checkpoint['net.0.weight'].size(1)  # Extract input size from checkpoint\n",
    "        self.reward_model = RewardMLP(input_dim - self.dim_action, self.dim_action).to(device)\n",
    "        self.reward_model.load_state_dict(checkpoint)\n",
    "        self.reward_model.load_state_dict(torch.load(reward_model_path, map_location=device, weights_only=False))\n",
    "        self.reward_model.eval()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # æ‰§è¡ŒåŸ envï¼Œä¸ç”¨åŸ reward\n",
    "        # Execute the original env, without the original reward\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)   \n",
    "        \n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # ä¿®æ”¹ action_tensor å½¢çŠ¶\n",
    "        # Modify action_tensor shape\n",
    "        if action_tensor.ndim == 1:\n",
    "            action_tensor = action_tensor.view(1, -1)\n",
    "\n",
    "        # è®¡ç®—å¥–åŠ±  \n",
    "        # Calculate reward\n",
    "        with torch.no_grad():\n",
    "            reward_tensor = self.reward_model(state_tensor, \n",
    "                                              action_tensor)\n",
    "        reward = reward_tensor.item()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "def reset(self, **kwargs):\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# 3. æ„é€  vectorized ç¯å¢ƒï¼Œå¹¶åº”ç”¨è‡ªå®šä¹‰ Wrapper\n",
    "# 3. Construct vectorized environment and apply custom Wrapper\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Log path\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = os.path.join(\"Training\", current_time)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Reward Model path\n",
    "MODEL_PATH = \"reward_net.pth\"\n",
    "\n",
    "\n",
    "#### Build env\n",
    "vec_env = make_vec_env(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_envs=8,\n",
    "    wrapper_class=lambda env: CustomRewardWrapper(env, MODEL_PATH, device=\"cpu\"),\n",
    "    monitor_dir=log_path\n",
    ")\n",
    "\n",
    "vec_env = VecMonitor(vec_env, log_path)\n",
    "\n",
    "\n",
    "#### Build PPO Model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=256,\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path\n",
    ")\n",
    "\n",
    "#### Training\n",
    "model.learn( \n",
    "    total_timesteps=50000,\n",
    "    # callback=[eval_callback, save_callback]\n",
    ")\n",
    "\n",
    "#### Save Model\n",
    "model.save(os.path.join(log_path, \"model_full_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01aac6",
   "metadata": {},
   "source": [
    "## **PPO-RLHF Testing**\n",
    "You can find record video in `Training\\2025-xx-xx\\video`  \n",
    "ä½ å¯ä»¥åœ¨è¿™ä¸ªä½ç½®æ‰¾åˆ°å½•åƒ `Training\\2025-xx-xx\\video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\2025-05-08_19-59-36\\model_full_training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\EPFL\\Course\\MA2\\[EE-568] Reinforcement Learning\\EE-568-RL\\PPO_RLHF\\Training\\2025-05-08_19-59-36\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 500.0\n",
      "Episode: 2 Score: 500.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "cur_dir = os.getcwd()\n",
    "pkg_dir = os.path.dirname(cur_dir)\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.append(pkg_dir)\n",
    "from PPO_Original import tools\n",
    "\n",
    "\n",
    "# PPO-RLHF model testing\n",
    "# log_path = \"Training\\\\2025-05-08_19-59-36\" #0000FF Change this to your log path\n",
    "print(\"log_path:\", log_path)\n",
    "PPO_Model_Path = os.path.join(log_path, \"model_full_training\")\n",
    "tools.test_model(\"PPO\", PPO_Model_Path, n_episodes=2, render = True, record=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
