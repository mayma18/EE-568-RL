{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514fce1",
   "metadata": {},
   "source": [
    "## **User Define Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5849e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Class:\n",
    "\n",
    "    # Internal Class\n",
    "    class Trajectory_Class:\n",
    "        def __init__(self, traj_series):\n",
    "            self.traj_list = traj_series\n",
    "            self.length = len(traj_series)\n",
    "\n",
    "        def get_single_traj(self, index):\n",
    "            return json.loads(self.traj_list[index])\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        # 原始数据 Original Data\n",
    "        self.trajs_prefer_list = []\n",
    "        self.trajs_reject_list = []\n",
    "\n",
    "        # 处理数据 Processed Data\n",
    "        self.traj_prefer_list_list_tensor = []\n",
    "        self.traj_reject_list_list_tensor = []\n",
    "\n",
    "        # 启动函数 Start Function\n",
    "        self.load_data(path)\n",
    "        self.convert(self.trajs_prefer_list, self.traj_prefer_list_list_tensor)   # data convert 数据转换\n",
    "        self.convert(self.trajs_reject_list, self.traj_reject_list_list_tensor)\n",
    "        print(\"Data loaded successfully\")\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        self.trajs_prefer_list = Data_Class.Trajectory_Class(data['preferred'])   # list data 数据\n",
    "        self.trajs_reject_list = Data_Class.Trajectory_Class(data['rejected'])    # list data 数据\n",
    "\n",
    "    def convert(self,\n",
    "                list_json: Trajectory_Class,\n",
    "                traj_list_list_tensor):\n",
    "\n",
    "        # 获取第0条轨迹的第0时刻样本来确定维度\n",
    "        # Get the first sample of the first trajectory to determine dimensions\n",
    "        sample = list_json.get_single_traj(0)[0]\n",
    "        state0 = np.array(sample['state'])\n",
    "        action0 = np.array(sample['action'])\n",
    "\n",
    "        # 获取 state action 维度\n",
    "        # Get the dimensions of state and action\n",
    "        self.dim_state = state0.size if state0.ndim == 0 else state0.shape[0]\n",
    "        self.dim_action = action0.size if action0.ndim == 0 else action0.shape[0]\n",
    "\n",
    "        # 数据批量转换 tensor\n",
    "        # Convert data to tensor in batches\n",
    "        for idx in range(list_json.length):\n",
    "            traj = list_json.get_single_traj(idx)\n",
    "            states, actions = [], []\n",
    "\n",
    "            for time_i in traj:\n",
    "                # 转换为 numpy，然后 torch tensor\n",
    "                # Convert to numpy, then torch tensor\n",
    "                state_np = np.array(time_i['state'])\n",
    "                action_np = np.array(time_i['action'])\n",
    "\n",
    "                state_t = torch.from_numpy(state_np).float()\n",
    "                action_t = torch.from_numpy(action_np).float()\n",
    "\n",
    "                # 如果是一维标量，要展开成长度1向量\n",
    "                # If it's a one-dimensional scalar, expand it into a length 1 vector\n",
    "                state_t = state_t.view(-1)\n",
    "                action_t = action_t.view(-1)\n",
    "\n",
    "                states.append(state_t)\n",
    "                actions.append(action_t)\n",
    "\n",
    "            # 将列表堆成张量 [L_i, dim]\n",
    "            # Stack the list into a tensor [L_i, dim]\n",
    "            states_tensor = torch.stack(states, dim=0)\n",
    "            actions_tensor = torch.stack(actions, dim=0)\n",
    "\n",
    "            # 将每条轨迹作为一个元组 (states, actions) 添加到列表中\n",
    "            # Add each trajectory as a tuple (states, actions) to the list\n",
    "            traj_list_list_tensor.append((states_tensor, actions_tensor))\n",
    "\n",
    "# ——— 数据集与加载器 ———\n",
    "# Dataset and DataLoader\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, pref, rej, gamma):\n",
    "        assert len(pref) == len(rej)\n",
    "        self.pref = pref\n",
    "        self.rej = rej\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.pref[idx], *self.rej[idx])\n",
    "\n",
    "# MLP 打分类 \n",
    "# MLP scoring model class\n",
    "class RewardMLP(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            #0000FF # 这里在构造神经网络，后去可能需要调整神经网络结构 \n",
    "            #0000FF # Here we construct the neural network, which may need to be adjusted later \n",
    "            nn.Linear(s_dim + a_dim, hidden_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [L_i, s_dim], a: [L_i, a_dim]\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fac82",
   "metadata": {},
   "source": [
    "**check the total reward (in cartpole environment, total reward is the same as episode length) of preferred and rejected trajectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "0 200 200\n",
      "1 200 144\n",
      "2 200 180\n",
      "3 200 194\n",
      "4 200 96\n",
      "5 200 125\n",
      "6 200 108\n",
      "7 200 106\n",
      "8 200 166\n",
      "9 173 97\n"
     ]
    }
   ],
   "source": [
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, len(Data.trajs_prefer_list.get_single_traj(i)), len(Data.trajs_reject_list.get_single_traj(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Reward Model Training**\n",
    "use `trajectory_pairs.csv` to train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/50 — Avg Loss: 1.0871\n",
      "Epoch 2/50 — Avg Loss: 0.9674\n",
      "Epoch 3/50 — Avg Loss: 0.8635\n",
      "Epoch 4/50 — Avg Loss: 0.7858\n",
      "Epoch 5/50 — Avg Loss: 0.7040\n",
      "Epoch 6/50 — Avg Loss: 0.6431\n",
      "Epoch 7/50 — Avg Loss: 0.5810\n",
      "Epoch 8/50 — Avg Loss: 0.5335\n",
      "Epoch 9/50 — Avg Loss: 0.4875\n",
      "Epoch 10/50 — Avg Loss: 0.4552\n",
      "Epoch 11/50 — Avg Loss: 0.4271\n",
      "Epoch 12/50 — Avg Loss: 0.4042\n",
      "Epoch 13/50 — Avg Loss: 0.3839\n",
      "Epoch 14/50 — Avg Loss: 0.3686\n",
      "Epoch 15/50 — Avg Loss: 0.3566\n",
      "Epoch 16/50 — Avg Loss: 0.3452\n",
      "Epoch 17/50 — Avg Loss: 0.3359\n",
      "Epoch 18/50 — Avg Loss: 0.3289\n",
      "Epoch 19/50 — Avg Loss: 0.3226\n",
      "Epoch 20/50 — Avg Loss: 0.3161\n",
      "Epoch 21/50 — Avg Loss: 0.3112\n",
      "Epoch 22/50 — Avg Loss: 0.3068\n",
      "Epoch 23/50 — Avg Loss: 0.3024\n",
      "Epoch 24/50 — Avg Loss: 0.2987\n",
      "Epoch 25/50 — Avg Loss: 0.2969\n",
      "Epoch 26/50 — Avg Loss: 0.2935\n",
      "Epoch 27/50 — Avg Loss: 0.2911\n",
      "Epoch 28/50 — Avg Loss: 0.2891\n",
      "Epoch 29/50 — Avg Loss: 0.2867\n",
      "Epoch 30/50 — Avg Loss: 0.2839\n",
      "Epoch 31/50 — Avg Loss: 0.2812\n",
      "Epoch 32/50 — Avg Loss: 0.2781\n",
      "Epoch 33/50 — Avg Loss: 0.2779\n",
      "Epoch 34/50 — Avg Loss: 0.2750\n",
      "Epoch 35/50 — Avg Loss: 0.2729\n",
      "Epoch 36/50 — Avg Loss: 0.2733\n",
      "Epoch 37/50 — Avg Loss: 0.2694\n",
      "Epoch 38/50 — Avg Loss: 0.2674\n",
      "Epoch 39/50 — Avg Loss: 0.2663\n",
      "Epoch 40/50 — Avg Loss: 0.2655\n",
      "Epoch 41/50 — Avg Loss: 0.2637\n",
      "Epoch 42/50 — Avg Loss: 0.2625\n",
      "Epoch 43/50 — Avg Loss: 0.2617\n",
      "Epoch 44/50 — Avg Loss: 0.2593\n",
      "Epoch 45/50 — Avg Loss: 0.2576\n",
      "Epoch 46/50 — Avg Loss: 0.2559\n",
      "Epoch 47/50 — Avg Loss: 0.2553\n",
      "Epoch 48/50 — Avg Loss: 0.2537\n",
      "Epoch 49/50 — Avg Loss: 0.2522\n",
      "Epoch 50/50 — Avg Loss: 0.2512\n",
      "🎉 Model has been saved to: reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ———— Hyperparameters ————\n",
    "gamma = 0.99      \n",
    "lr = 1e-4          \n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "# ———— Load Data ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "# 自定义 collate_fn，保留变长序列\n",
    "# Custom collate_fn to keep variable-length sequences\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# ——— Training Preperation ———\n",
    "dataset = PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64) # 实例化 神经网络 MLP\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ——— Training Loop ———\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # 计算 prefer 轨迹的回报\n",
    "        # Calculate the return for preferred trajectories\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # 计算 reject 轨迹的回报\n",
    "        # Calculate the return for rejected trajectories\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        \n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ——— Save Model ———\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"🎉 Model has been saved to: reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Reward Model Testing**\n",
    "Load `trajectory_pairs.csv` to see if the total reward matches what the `MLP reward model` predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(12.2832) tensor(11.5129)\n",
      "1 tensor(11.5982) tensor(8.9034)\n",
      "2 tensor(12.5000) tensor(10.9671)\n",
      "3 tensor(12.3189) tensor(10.8672)\n",
      "4 tensor(12.3967) tensor(6.5947)\n",
      "5 tensor(12.2223) tensor(8.9502)\n",
      "6 tensor(12.2360) tensor(6.0135)\n",
      "7 tensor(12.1494) tensor(5.9533)\n",
      "8 tensor(11.9423) tensor(11.1330)\n",
      "9 tensor(11.5264) tensor(7.4788)\n"
     ]
    }
   ],
   "source": [
    "# ——— Load MLP Reward Model ———\n",
    "reward_net_loaded = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "reward_net_loaded.eval()\n",
    "\n",
    "# Load .csv Data\n",
    "for i in range(10):\n",
    "    traj_prefer_json = Data.trajs_prefer_list.get_single_traj(i)\n",
    "    traj_reject_json = Data.trajs_reject_list.get_single_traj(i)\n",
    "\n",
    "    # convert to tensor\n",
    "    states_prefer  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                  for step in traj_prefer_json], dim=0)    # [L, s_dim]\n",
    "    actions_prefer = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_prefer_json], dim=0)  # [L, a_dim]\n",
    "    states_reject  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, s_dim]\n",
    "    actions_reject = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "    # 计算 prefer 轨迹的回报\n",
    "    # Calculate the return for preferred trajectories\n",
    "    with torch.no_grad():\n",
    "        r_pref = reward_net_loaded(states_prefer, actions_prefer)          \n",
    "\n",
    "    # 计算总回报\n",
    "    # Calculate total return\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_pref.size(0))])\n",
    "    total_return_prefer = (r_pref * discounts).sum()\n",
    "\n",
    "    # 计算 reject 轨迹的回报\n",
    "    # Calculate the return for rejected trajectories\n",
    "    with torch.no_grad():\n",
    "        r_rj = reward_net_loaded(states_reject, actions_reject)          \n",
    "\n",
    "    # 计算总回报\n",
    "    # Calculate total return\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))])\n",
    "    total_return_reject = (r_rj * discounts).sum()\n",
    "\n",
    "    print(i, total_return_prefer, total_return_reject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951354",
   "metadata": {},
   "source": [
    "## **PPO-RLHF Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\2025-05-08_21-21-23\\PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | 3.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 3437     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.6        |\n",
      "|    ep_rew_mean          | 3.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1770        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010768814 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.0244      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.522       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.4        |\n",
      "|    ep_rew_mean          | 5.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1460        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014078374 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.181       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.535       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 45.6        |\n",
      "|    ep_rew_mean          | 6.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1372        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011568268 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.814       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 59.8         |\n",
      "|    ep_rew_mean          | 8.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1313         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075433906 |\n",
      "|    clip_fraction        | 0.0745       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.294        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.758        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73.9        |\n",
      "|    ep_rew_mean          | 10.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1286        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011101447 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.471       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 82.6        |\n",
      "|    ep_rew_mean          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1262        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015353655 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0939      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# 2. 定义一个 Wrapper，在 step 里用你的 MLP 计算 reward\n",
    "# 2. Define a Wrapper that uses your MLP to calculate the reward in step\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_model_path, device=\"cpu\"):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # state dimension\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "\n",
    "        # action dimension\n",
    "        try:                  self.dim_action = env.action_space.shape[0]\n",
    "        except IndexError:    self.dim_action = 1\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Ensure the input dimensions match the checkpoint\n",
    "        checkpoint = torch.load(reward_model_path, map_location=device, weights_only=False)\n",
    "        input_dim = checkpoint['net.0.weight'].size(1)  # Extract input size from checkpoint\n",
    "        self.reward_model = RewardMLP(input_dim - self.dim_action, self.dim_action).to(device)\n",
    "        self.reward_model.load_state_dict(checkpoint)\n",
    "        self.reward_model.load_state_dict(torch.load(reward_model_path, map_location=device, weights_only=False))\n",
    "        self.reward_model.eval()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 执行原 env，不用原 reward\n",
    "        # Execute the original env, without the original reward\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)   \n",
    "        \n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # 修改 action_tensor 形状\n",
    "        # Modify action_tensor shape\n",
    "        if action_tensor.ndim == 1:\n",
    "            action_tensor = action_tensor.view(1, -1)\n",
    "\n",
    "        # 计算奖励  \n",
    "        # Calculate reward\n",
    "        with torch.no_grad():\n",
    "            reward_tensor = self.reward_model(state_tensor, \n",
    "                                              action_tensor)\n",
    "        reward = reward_tensor.item()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "def reset(self, **kwargs):\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# 3. 构造 vectorized 环境，并应用自定义 Wrapper\n",
    "# 3. Construct vectorized environment and apply custom Wrapper\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Log path\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = os.path.join(\"Training\", current_time)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Reward Model path\n",
    "MODEL_PATH = \"reward_net.pth\"\n",
    "\n",
    "\n",
    "#### Build env\n",
    "vec_env = make_vec_env(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_envs=8,\n",
    "    wrapper_class=lambda env: CustomRewardWrapper(env, MODEL_PATH, device=\"cpu\"),\n",
    "    monitor_dir=log_path\n",
    ")\n",
    "\n",
    "vec_env = VecMonitor(vec_env, log_path)\n",
    "\n",
    "\n",
    "#### Build PPO Model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=256,\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path\n",
    ")\n",
    "\n",
    "#### Training\n",
    "model.learn( \n",
    "    total_timesteps=50000,\n",
    "    # callback=[eval_callback, save_callback]\n",
    ")\n",
    "\n",
    "#### Save Model\n",
    "model.save(os.path.join(log_path, \"model_full_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01aac6",
   "metadata": {},
   "source": [
    "## **PPO-RLHF Testing**\n",
    "You can find record video in `Training\\2025-xx-xx\\video`  \n",
    "你可以在这个位置找到录像 `Training\\2025-xx-xx\\video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\2025-05-08_19-59-36\\model_full_training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\EPFL\\Course\\MA2\\[EE-568] Reinforcement Learning\\EE-568-RL\\PPO_RLHF\\Training\\2025-05-08_19-59-36\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 500.0\n",
      "Episode: 2 Score: 500.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "cur_dir = os.getcwd()\n",
    "pkg_dir = os.path.dirname(cur_dir)\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.append(pkg_dir)\n",
    "from PPO_Original import tools\n",
    "\n",
    "\n",
    "# PPO-RLHF model testing\n",
    "# log_path = \"Training\\\\2025-05-08_19-59-36\" #0000FF Change this to your log path\n",
    "print(\"log_path:\", log_path)\n",
    "PPO_Model_Path = os.path.join(log_path, \"model_full_training\")\n",
    "tools.test_model(\"PPO\", PPO_Model_Path, n_episodes=2, render = True, record=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
